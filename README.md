# pyspark
learn how to use spark with python

###尝试安装spark，并连接spark和python。
#1.下载了spark包，大概300M的样子。
#2.解压缩，找到里面python目录下的setup.py
#3.在powershell里输入python setup.py install
#4.用pip list看是否安装好了。
#5.输入python，然后import pyspark，看是否能够运行pyspark。
#6.警告，spark会占不少内存，低于4G者先加内存条。（安装上后，看看任务管理器，里面的内存还剩多少。）


PS G:\spark-2.1.0-bin-hadoop2.7\python> python setup.py install
Could not import pypandoc - required to package PySpark
running install
running bdist_egg
running egg_info


py4j.java_gateway: module references __file__
py4j.tests.java_gateway_test: module references __file__
py4j.tests.java_tls_test: module references __file__
creating g:\programdata\anaconda2\lib\site-packages\py4j-0.10.4-py2.7.egg
Extracting py4j-0.10.4-py2.7.egg to g:\programdata\anaconda2\lib\site-packages
Adding py4j 0.10.4 to easy-install.pth file

Installed g:\programdata\anaconda2\lib\site-packages\py4j-0.10.4-py2.7.egg
Finished processing dependencies for pyspark==2.1.0+hadoop2.7
PS G:\spark-2.1.0-bin-hadoop2.7\python> pip install pyspark
Requirement already satisfied: pyspark in g:\programdata\anaconda2\lib\site-packages\pyspark-2.1.0+hadoop2.7-py2.7.egg
Requirement already satisfied: py4j==0.10.4 in g:\programdata\anaconda2\lib\site-packages\py4j-0.10.4-py2.7.egg (from pyspark)
PS G:\spark-2.1.0-bin-hadoop2.7\python> pip list


pyOpenSSL (16.2.0)
pyparsing (2.1.4)
pyspark (2.1.0+hadoop2.7)
pytest (3.0.5)
python-dateutil (2.6.0)









